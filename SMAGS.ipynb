{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47064913",
   "metadata": {},
   "source": [
    "# Ovary Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e631c43",
   "metadata": {},
   "source": [
    "### Import necesaary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06cc06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings globally (including future, deprecation, runtime, sklearn, etc.)\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time  # For timing operations\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize  # For optimization tasks\n",
    "from joblib import Parallel, delayed  # For parallel processing\n",
    "from sklearn.linear_model import LogisticRegression, Lasso  # For logistic regression and Lasso regression\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, log_loss, accuracy_score, recall_score, confusion_matrix, f1_score, precision_score  # For classification metrics\n",
    "from sklearn.model_selection import train_test_split, KFold  # For splitting datasets and cross-validation\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "from sklearn.svm import SVR  # For Support Vector Regression\n",
    "from sklearn.ensemble import RandomForestRegressor  # For Random Forest regression\n",
    "from sklearn.utils.class_weight import compute_class_weight  # For handling class imbalance\n",
    "import seaborn as sns  # For advanced data visualization\n",
    "\n",
    "# Set a random state for reproducibility\n",
    "randomstate = 1729\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98109254",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c8ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ovary Cancer dataset\n",
    "df = pd.read_csv(\"final.data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9a836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features (X) and target (y) from dataset\n",
    "X = df.drop(columns=['ovar_cancer'])\n",
    "y = df['ovar_cancer']\n",
    "\n",
    "# Convert to float and ensure types are correct\n",
    "X = X.astype(float)\n",
    "y = y.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7557e27",
   "metadata": {},
   "source": [
    "### Necessary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf14b8a",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b92907",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Choosing features that have the highest sensitivity for pre-specified specificity.\n",
    "\n",
    "Input:\n",
    "- X: Pandas DataFrame of features.\n",
    "- y: Pandas Series of classes (class column).\n",
    "- SP: The value for specificity.\n",
    "- lower_bound: Optional boundaries for the coefficients of the feature (should be of dimension X.shape[1] + 1).\n",
    "- num_features_to_keep: The number of features to keep that maximize sensitivity at the given specificity.\n",
    "\n",
    "Output:\n",
    "- List of features (with the length of num_features_to_keep).\n",
    "'''\n",
    "\n",
    "def feature_selection(X, y, SP, lower_bound=None, num_features_to_keep=2):\n",
    "    Features = list(X.columns)  # List of all feature names\n",
    "    One_Top = []  # List to hold selected features\n",
    "    \n",
    "    def sensitivity_score(feature):\n",
    "        # Calculate sensitivity for a given feature\n",
    "        if len(One_Top) == 0:\n",
    "            return SMAGS(X=X.loc[:, [feature]], y=y, SP=SP, lower_bound=lower_bound)['Sensitivity']\n",
    "        else:\n",
    "            return SMAGS(X=X.loc[:, np.append(feature, One_Top)], y=y, SP=SP, lower_bound=lower_bound)['Sensitivity']\n",
    "\n",
    "    while len(One_Top) < num_features_to_keep:\n",
    "        # Calculate sensitivity scores for all features in parallel\n",
    "        results = Parallel(n_jobs=-2)(delayed(sensitivity_score)(feature) for feature in Features)\n",
    "        best_feature = Features[np.argmax(results)]  # Get the feature with the highest sensitivity\n",
    "\n",
    "        Features.remove(best_feature)  # Remove the best feature from consideration\n",
    "        One_Top.append(best_feature)  # Add the best feature to the selected list\n",
    "\n",
    "    return One_Top  # Return the list of selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187aafe8",
   "metadata": {},
   "source": [
    "**Modifying `feature_selection` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af14d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely convert complex numbers to real\n",
    "def safe_real(x):\n",
    "    return np.real(x) if np.iscomplexobj(x) else x\n",
    "\n",
    "# Modify the SMAGS function to handle complex numbers\n",
    "def complex_safe_smags(*args, **kwargs):\n",
    "    result = SMAGS(*args, **kwargs)\n",
    "    return pd.Series({k: safe_real(v) for k, v in result.items()})\n",
    "\n",
    "# Update feature_selection to use the complex-safe SMAGS\n",
    "def complex_safe_feature_selection(X, y, SP, lower_bound=None, num_features_to_keep=2):\n",
    "    Features = list(X.columns)\n",
    "    One_Top = []\n",
    "    \n",
    "    def sensitivity_score(feature):\n",
    "        if len(One_Top) == 0:\n",
    "            return safe_real(complex_safe_smags(X=X.loc[:, [feature]], y=y, SP=SP, lower_bound=lower_bound)['Sensitivity'])\n",
    "        else:\n",
    "            return safe_real(complex_safe_smags(X=X.loc[:, np.append(feature, One_Top)], y=y, SP=SP, lower_bound=lower_bound)['Sensitivity'])\n",
    "    \n",
    "    while len(One_Top) < num_features_to_keep:\n",
    "        results = [sensitivity_score(feature) for feature in Features]\n",
    "        best_feature = Features[np.argmax(results)]\n",
    "        Features.remove(best_feature)\n",
    "        One_Top.append(best_feature)\n",
    "    \n",
    "    return One_Top\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505e22d",
   "metadata": {},
   "source": [
    "##### SMAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d58ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sensitivity Maximization At Given Specificity (SMAGS):\n",
    "\n",
    "Choosing features that have the highest sensitivity for pre-specified specificity.\n",
    "\n",
    "Input:\n",
    "- X: Features as a Pandas DataFrame.\n",
    "- y: Labels as a Pandas Series of classes (0 and 1).\n",
    "- SP: Threshold value for specificity (default is 0.985).\n",
    "- lower_bound: Optional boundaries for the coefficients (should be of dimension X.shape[1] + 1).\n",
    "\n",
    "Output:\n",
    "A Series with coefficients of the logistic regression that maximizes sensitivity,\n",
    "including intercept, threshold, specificity, sensitivity, optimization method, tolerance, jacobian approach, and AIC.\n",
    "'''\n",
    "\n",
    "def SMAGS(X, y, SP=0.985, lower_bound=None, rand=randomstate):\n",
    "    n_features = X.shape[1]\n",
    "    log = LogisticRegression(random_state=rand).fit(X, y)  # Fit logistic regression model\n",
    "    all_coefs = np.append(log.coef_[0], log.intercept_)  # Get coefficients and intercept\n",
    "    X1 = X.assign(I=1)  # Add intercept term to feature set\n",
    "\n",
    "    def sig(M):\n",
    "        # Sigmoid function\n",
    "        return 1 / (1 + np.exp(-M))\n",
    "\n",
    "    def custom_loss(coefs):\n",
    "        # Custom loss function to maximize sensitivity\n",
    "        m = np.matmul(X1, coefs)\n",
    "        z = sig(m.astype(float))\n",
    "        if len(((1 - y) * z)[(1 - y) * z != 0]) != 0:\n",
    "            threshold = np.quantile(((1 - y) * z)[(1 - y) * z != 0], SP)\n",
    "        else:\n",
    "            threshold = 0\n",
    "        y_hat = 1 * (z > threshold)  # Predicted labels based on threshold\n",
    "        loss1 = np.dot(y_hat, y) / sum(y)  # Sensitivity\n",
    "        return -loss1  # Return negative sensitivity for minimization\n",
    "\n",
    "    # Set bounds for coefficients\n",
    "    bnds = [(lower_bound, None) for _ in range(n_features)]\n",
    "    bnds.append((None, None))  # Intercept bound\n",
    "\n",
    "    # Define optimization methods, tolerances, and jacobian approaches\n",
    "    opt_methods = ['Nelder-Mead', 'Powell', 'CG', 'BFGS', 'L-BFGS-B', 'TNC', 'COBYLA', 'SLSQP', 'trust-constr']\n",
    "    tolerance = [1, 5e-1, 1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "    jacob_approach = ['2-point', '3-point', 'cs']\n",
    "\n",
    "    def process_params(met, tol, jac):\n",
    "        # Process parameters for optimization\n",
    "        result = minimize(fun=custom_loss, x0=np.abs(all_coefs), method=met, tol=tol, bounds=bnds, jac=jac)\n",
    "        a = np.matmul(X1, result.x)  # Predicted values\n",
    "        b = sig(a.astype(float))\n",
    "        \n",
    "        if len(((1 - y) * b)[(1 - y) * b != 0]) != 0:\n",
    "            c = np.quantile(((1 - y) * b)[(1 - y) * b != 0], SP)\n",
    "        else:\n",
    "            c = 0\n",
    "        d = 1 * (b > c)  # Predicted labels based on updated threshold\n",
    "\n",
    "        # Create a Series to hold results\n",
    "        row = pd.Series(index=np.append(np.array(['B' + str(i) for i in range(1, n_features + 1)]),\n",
    "                                         np.array(['B0', 'method', 'tolerance', 'jacobian', 'threshold',\n",
    "                                                   'Specificity', 'Sensitivity', 'AIC'])))\n",
    "        row[0:(n_features + 1)] = result.x  # Coefficients and intercept\n",
    "        row['method'] = met\n",
    "        row['tolerance'] = tol\n",
    "        row['jacobian'] = jac\n",
    "        row['threshold'] = SP\n",
    "        row['Specificity'] = np.dot(1 - d, 1 - y) / sum(1 - y)  # Calculate specificity\n",
    "        row['Sensitivity'] = np.dot(d, y) / sum(y)  # Calculate sensitivity\n",
    "        row['AIC'] = 2 * ((n_features + 1) - log_loss(y, sig(np.matmul(X1, result.x).astype(float))))  # Calculate AIC\n",
    "\n",
    "        return row\n",
    "\n",
    "    # Run the optimization in parallel for different methods, tolerances, and Jacobian approaches\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_params)(met, tol, jac) for met in opt_methods\n",
    "                                   for tol in tolerance for jac in jacob_approach)\n",
    "\n",
    "    max_sn = pd.DataFrame(results)\n",
    "    # Filter for the maximum sensitivity and then minimize AIC\n",
    "    A = max_sn[max_sn.Sensitivity == max_sn.Sensitivity.max()]\n",
    "    return A[A.AIC == A.AIC.min()].iloc[0]  # Return the optimal result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5bd6e",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554eb822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Features=['lgca125', 'lgsil_4r_pg_ml', 'lgstnfri_pg_ml', 'lgegf_pg_ml', 'lgil_1ra_pg_ml', 'lgmip_1b_pg_ml', 'lgmcp_1_pg_ml']\n",
      "Fold 1: Accuracy=0.28, Sensitivity=1.00, Specificity=0.00, AUC=0.71, Sens@SP=98.5%=0.091\n",
      "Fold 2: Features=['lgca125', 'lgeotaxin_pg_ml', 'lgmdc_pg_ml', 'lgil_8_pg_ml', 'lgegf_pg_ml', 'lgfgf_2_pg_ml', 'lgmip_1b_pg_ml']\n",
      "Fold 2: Accuracy=0.80, Sensitivity=0.04, Specificity=1.00, AUC=0.74, Sens@SP=98.5%=0.167\n",
      "Fold 3: Features=['lgca125', 'lgeotaxin_pg_ml', 'lgg_csf_pg_ml', 'lgmip_1b_pg_ml', 'lgfgf_2_pg_ml', 'lgil_1ra_pg_ml', 'lgmdc_pg_ml']\n",
      "Fold 3: Accuracy=0.66, Sensitivity=0.55, Specificity=0.70, AUC=0.65, Sens@SP=98.5%=0.172\n",
      "Fold 4: Features=['lgca125', 'lgeotaxin_pg_ml', 'lgegf_pg_ml', 'lgil_1ra_pg_ml', 'lgstnfri_pg_ml', 'lgip_10_pg_ml', 'lgg_csf_pg_ml']\n",
      "Fold 4: Accuracy=0.78, Sensitivity=0.13, Specificity=1.00, AUC=0.67, Sens@SP=98.5%=0.200\n",
      "Fold 5: Features=['lgca125', 'lgeotaxin_pg_ml', 'lgmip_1b_pg_ml', 'lgcrp_pg_ml', 'lgmcp_1_pg_ml', 'lgtnf_b_pg_ml', 'lgip_10_pg_ml']\n",
      "Fold 5: Accuracy=0.26, Sensitivity=1.00, Specificity=0.00, AUC=0.72, Sens@SP=98.5%=0.167\n",
      "\n",
      "Mean Accuracy: 0.56 ± 0.24\n",
      "Mean Sensitivity: 0.55 ± 0.41\n",
      "Mean Specificity: 0.54 ± 0.45\n",
      "Mean AUC: 0.70 ± 0.03\n",
      "Mean Sensitivity @ 98.5% Specificity: 0.159 ± 0.036\n",
      "\n",
      "Selected features per fold:\n",
      "Fold 1: ['lgca125', 'lgsil_4r_pg_ml', 'lgstnfri_pg_ml', 'lgegf_pg_ml', 'lgil_1ra_pg_ml', 'lgmip_1b_pg_ml', 'lgmcp_1_pg_ml']\n",
      "Fold 2: ['lgca125', 'lgeotaxin_pg_ml', 'lgmdc_pg_ml', 'lgil_8_pg_ml', 'lgegf_pg_ml', 'lgfgf_2_pg_ml', 'lgmip_1b_pg_ml']\n",
      "Fold 3: ['lgca125', 'lgeotaxin_pg_ml', 'lgg_csf_pg_ml', 'lgmip_1b_pg_ml', 'lgfgf_2_pg_ml', 'lgil_1ra_pg_ml', 'lgmdc_pg_ml']\n",
      "Fold 4: ['lgca125', 'lgeotaxin_pg_ml', 'lgegf_pg_ml', 'lgil_1ra_pg_ml', 'lgstnfri_pg_ml', 'lgip_10_pg_ml', 'lgg_csf_pg_ml']\n",
      "Fold 5: ['lgca125', 'lgeotaxin_pg_ml', 'lgmip_1b_pg_ml', 'lgcrp_pg_ml', 'lgmcp_1_pg_ml', 'lgtnf_b_pg_ml', 'lgip_10_pg_ml']\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "accs, sns, sps, aucs, sens_at_sp985s = [], [], [], [], []\n",
    "selected_features_per_fold = []\n",
    "n_selecting_features = 7\n",
    "SP = 0.985\n",
    "n_fold = 5\n",
    "\n",
    "kfold = KFold(n_splits=n_fold, shuffle=True, random_state=randomstate)\n",
    "\n",
    "for fold_idx, (train, test) in enumerate(kfold.split(X), 1):\n",
    "    # 1. Feature selection on training data only\n",
    "    fold_selected_features = complex_safe_feature_selection(\n",
    "        X.loc[X.index[train]], y.loc[y.index[train]],\n",
    "        SP=SP, num_features_to_keep=n_selecting_features\n",
    "    )\n",
    "    selected_features_per_fold.append(fold_selected_features)\n",
    "\n",
    "    # 2. Fit SMAGS on training data with selected features\n",
    "    A = SMAGS(\n",
    "        X.loc[X.index[train], fold_selected_features],\n",
    "        y.loc[y.index[train]], SP\n",
    "    )\n",
    "\n",
    "    # 3. Predict on test data with selected features\n",
    "    X_test = X.loc[X.index[test], fold_selected_features].assign(I=1)\n",
    "    coefs = np.array(A[:len(fold_selected_features) + 1], dtype=float)\n",
    "    probs = sigmoid(np.matmul(X_test.values.astype(float), coefs))\n",
    "    threshold = A['threshold'] if 'threshold' in A else 0.5\n",
    "    y_pred = (probs > threshold).astype(int)\n",
    "    y_true = y.loc[y.index[test]]\n",
    "\n",
    "    # 4. Calculate standard metrics\n",
    "    try:\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        sn = recall_score(y_true, y_pred, zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sp = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        auc_ = roc_auc_score(y_true, probs)\n",
    "    except Exception as e:\n",
    "        print(f\"Metric calculation error on Fold {fold_idx}: {e}\")\n",
    "        acc = sn = sp = auc_ = np.nan\n",
    "\n",
    "    # 5. Sensitivity at 98.5% specificity (main result!)\n",
    "    try:\n",
    "        fpr_full, tpr_full, thresholds = roc_curve(y_true, probs)\n",
    "        idx = np.where(1 - fpr_full >= SP)[0]\n",
    "        if len(idx) > 0:\n",
    "            sens_at_sp = tpr_full[idx[-1]]  # highest sensitivity where specificity >= SP\n",
    "        else:\n",
    "            sens_at_sp = np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"ROC curve error on Fold {fold_idx}: {e}\")\n",
    "        sens_at_sp = np.nan\n",
    "\n",
    "    # 6. Append metrics for this fold\n",
    "    accs.append(acc)\n",
    "    sns.append(sn)\n",
    "    sps.append(sp)\n",
    "    aucs.append(auc_)\n",
    "    sens_at_sp985s.append(sens_at_sp)\n",
    "\n",
    "    # 7. Print results for this fold\n",
    "    print(f\"Fold {fold_idx}: Features={fold_selected_features}\")\n",
    "    print(f\"Fold {fold_idx}: Accuracy={acc:.2f}, Sensitivity={sn:.2f}, Specificity={sp:.2f}, \"\n",
    "          f\"AUC={auc_:.2f}, Sens@SP=98.5%={sens_at_sp:.3f}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nMean Accuracy: {np.nanmean(accs):.2f} ± {np.nanstd(accs):.2f}\")\n",
    "print(f\"Mean Sensitivity: {np.nanmean(sns):.2f} ± {np.nanstd(sns):.2f}\")\n",
    "print(f\"Mean Specificity: {np.nanmean(sps):.2f} ± {np.nanstd(sps):.2f}\")\n",
    "print(f\"Mean AUC: {np.nanmean(aucs):.2f} ± {np.nanstd(aucs):.2f}\")\n",
    "print(f\"Mean Sensitivity @ 98.5% Specificity: {np.nanmean(sens_at_sp985s):.3f} ± {np.nanstd(sens_at_sp985s):.3f}\")\n",
    "\n",
    "print(\"\\nSelected features per fold:\")\n",
    "for i, feats in enumerate(selected_features_per_fold, 1):\n",
    "    print(f\"Fold {i}: {feats}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
